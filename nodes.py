from langchain_core.messages import HumanMessage
from state import TrendAgentState
import prompts
from config import LLM, TAVILY_CLIENT

# ë…¸ë“œ 1: íŠ¸ë Œë“œ ë¦¬ì„œì²˜
def execute_researcher(state: TrendAgentState):
    print("--- ğŸ” íŠ¸ë Œë“œ ë¦¬ì„œì²˜ ì‹¤í–‰ ---")
    keyword = state['keyword']
    response = TAVILY_CLIENT.search(query=f"'{keyword}'ê°€ ì™œ íŠ¸ë Œë“œì¸ê°€?", search_depth="advanced", max_results=5)
    state['search_results'] = response['results']
    return state

# ë…¸ë“œ 2: íŠ¸ë Œë“œ ë¶„ì„ê°€
def execute_analyst(state: TrendAgentState):
    print("--- ğŸ“Š íŠ¸ë Œë“œ ë¶„ì„ê°€ ì‹¤í–‰ ---")
    search_results_text = "\n\n".join([f"URL: {res['url']}\nContent: {res['content']}" for res in state['search_results']])
    
    prompt = prompts.ANALYST_PROMPT_TEMPLATE.format(
        keyword=state['keyword'],
        search_results_text=search_results_text
    )
    
    response = LLM.invoke([HumanMessage(content=prompt)])
    state['summary'] = response.content
    return state

# ë…¸ë“œ 3: ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ê¸°
def execute_classifier(state: TrendAgentState):
    print("--- ğŸ·ï¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ê¸° ì‹¤í–‰ ---")
    prompt = prompts.CLASSIFIER_PROMPT_TEMPLATE.format(summary=state['summary'])
    
    response = LLM.invoke([HumanMessage(content=prompt)])
    try:
        # LLMì´ ìƒì„±í•œ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‹¤ì œ Python ë¦¬ìŠ¤íŠ¸ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
        state['categories'] = eval(response.content)
    except:
        state['categories'] = ["ê¸°íƒ€"] # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
    return state

# ë…¸ë“œ 4: ì½˜í…ì¸  ìƒì„±ê°€
def execute_creator(state: TrendAgentState):
    print("--- âœï¸ ì½˜í…ì¸  ìƒì„±ê°€ ì‹¤í–‰ ---")
    categories_str = ", ".join(state['categories'])
    
    # [ì¶”ê°€] ë¦¬ì„œì²˜ê°€ ìˆ˜ì§‘í•œ ì›ë³¸ ë°ì´í„°ë¥¼ í”„ë¡¬í”„íŠ¸ì— ë„£ê¸° ì¢‹ê²Œ ê°€ê³µí•©ë‹ˆë‹¤.
    search_results_str = "\n\n".join(
        [f"ì¶œì²˜ URL: {res['url']}\në‚´ìš©: {res['content']}" for res in state['search_results']]
    )

    # [ìˆ˜ì •] formatì— search_results_strë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    prompt = prompts.CREATOR_PROMPT_TEMPLATE.format(
        keyword=state['keyword'],
        summary=state['summary'],
        categories_str=categories_str,
        search_results_str=search_results_str # ìƒˆë¡œ ì¶”ê°€ëœ ë³€ìˆ˜
    )
    
    response = LLM.invoke([HumanMessage(content=prompt)])
    state['blog_post'] = response.content
    return state